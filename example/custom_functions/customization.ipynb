{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crimson.executable_types_beta.mock import TensorMock\n",
    "from crimson.executable_types_beta.types import *\n",
    "from inspect import currentframe\n",
    "from crimson.executable_types_beta.executor import AnnotationExecutor\n",
    "from crimson.executable_types_beta.data_model import ExecutionResult\n",
    "from crimson.executable_types_beta.utils import (\n",
    "    get_meta_annotations,\n",
    "    get_meta_annotation,\n",
    "    generate_meta_dict,\n",
    "    push_execution_result,\n",
    "    get_names\n",
    ")\n",
    "from crimson.executable_types_beta.test import test_meta_dict\n",
    "from crimson.types_beta.addon import TypesPack\n",
    "from crimson.ast_dev_tool import collect_nodes, print_node\n",
    "import ast\n",
    "\n",
    "\n",
    "import inspect\n",
    "\n",
    "def print_object(obj):\n",
    "    print(inspect.getsource(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloatRange(\n",
    "    float,\n",
    "    TypesPack[T]\n",
    "):\n",
    "\tpass\n",
    "\n",
    "class IntRange(\n",
    "    float,\n",
    "    TypesPack[T]\n",
    "):\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnnotationExecutor.set_config(\n",
    "    push_result=True,\n",
    "    raise_error=False,\n",
    ")\n",
    "\n",
    "AnnotationExecutor.env = currentframe().f_locals# {'TensorMock':TensorMock}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = TensorMock.ones((4, 6))\n",
    "array2 = TensorMock.ones((8, 10))\n",
    "array3 = TensorMock.ones((4, 8))\n",
    "array4 = TensorMock.ones((4, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@AnnotationExecutor.with_executable\n",
    "def func_with_error(\n",
    "    array1: TensorMock[List, \"b\", \"c\"], \n",
    "    array2: TensorMock[List, \"d == 2*b\", \"e == b+2*c\"],\n",
    "    array3: TensorMock\n",
    ") -> TensorMock[List, \"b\", \"f == 3 * b\"]:\n",
    "    return array4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to push the results, but the `ArgDetails` are too big?\n",
    "\n",
    "In this case, you'd better customize the `push_execution_result` function.\n",
    "\n",
    "Check the default `push_execution_result` first.\n",
    "\n",
    "``` python\n",
    "def push_execution_result(\n",
    "    func: FunctionType,\n",
    "    execution_results: Dict[str, ExecutionResult],\n",
    "    execution_result: ExecutionResult,\n",
    "    *args,\n",
    "    **kwargs\n",
    "):\n",
    "    execution_results[func.__name__] = execution_result\n",
    "```\n",
    "\n",
    "the `ExecutionResult` includes the arguments passed to the function and the output of the function.\n",
    "\n",
    "If all of their references are stored in `execution_results`,\n",
    "they are not collected by the garbage collector.\n",
    "\n",
    "If we want to analyze or test a DL model, the tensors are too to be stored.\n",
    "The shape of tensors would be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_to_shape(arg_details: ArgsDetails_):\n",
    "\treturn {key: value.shape for key, value in arg_details.items() if isinstance(value, TensorMock)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'array1': [[1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1]],\n",
       " 'array2': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       " 'array3': [[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1]],\n",
       " 'return': [[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_with_error(\n",
    "    array1,\n",
    "    array2,\n",
    "    array3\n",
    ")\n",
    "\n",
    "AnnotationExecutor.last_execution_result.args_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_push_execution_result(\n",
    "    func,\n",
    "    execution_results,\n",
    "    execution_result: ExecutionResult,\n",
    "    *args,\n",
    "    **kwargs\n",
    "):\n",
    "    execution_result.args_detail = convert_tensor_to_shape(execution_result.args_detail)\n",
    "    execution_results[func.__name__] = execution_result\n",
    "\n",
    "\n",
    "AnnotationExecutor.set_functions(\n",
    "    push_execution_result=my_push_execution_result\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'array1': (4, 6), 'array2': (8, 10), 'array3': (4, 8), 'return': (4, 8)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_with_error(\n",
    "    array1,\n",
    "    array2,\n",
    "    array3\n",
    ")\n",
    "\n",
    "AnnotationExecutor.last_execution_result.args_detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Dict\n",
    "\n",
    "You might want to test more than .shape.\n",
    "In this case, you need to customize the `generate_meta_dict` and `get_meta_annotations`.\n",
    "\n",
    "I don't recommend to implement it from the skratch.\n",
    "Let's make a customized one using the pre-defined ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@AnnotationExecutor.with_executable\n",
    "def forward(\n",
    "        tensor: TensorMock[\"b\", \"c\", \"h\", \"w\"],\n",
    "        temperature: FloatRange[float, 0, 1],\n",
    "        depth: IntRange[int, 3, 12],\n",
    "        extra_arg = 1\n",
    "    ):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscript_nodes = collect_nodes(forward, ast.Subscript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_annotations(\n",
    "    func: ast.FunctionType,\n",
    ") -> MetaAnnotations_[Dict[ArgumentName_[str], MetaAnnotation_[List[str] | None]]]:\n",
    "    meta_annotations = {}\n",
    "    func_node = collect_nodes(func, ast.FunctionDef)[0]\n",
    "    arg_nodes = collect_nodes(func, ast.arg)\n",
    "    for arg_node in arg_nodes:\n",
    "        arg_name = arg_node.arg\n",
    "        meta_annotation = get_meta_annotation(arg_node.annotation)\n",
    "\n",
    "        try:\n",
    "            if arg_node.annotation.value.id in [FloatRange.__name__, IntRange.__name__]:\n",
    "                meta_annotation[0] = f\"{arg_name} >= {meta_annotation[0]}\"\n",
    "                meta_annotation[1] = f\"{arg_name} <= {meta_annotation[1]}\"\n",
    "        except Exception as e:\n",
    "               e\n",
    "            \n",
    "\n",
    "        meta_annotations[arg_node.arg] = meta_annotation\n",
    "\n",
    "    if hasattr(func_node, \"returns\"):\n",
    "        meta_annotations[\"return\"] = get_meta_annotation(func_node.returns)\n",
    "\n",
    "    return meta_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Type\n",
    "\n",
    "from pydantic import *\n",
    "from pydantic.fields import FieldInfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_meta_dict(\n",
    "    args_details: ArgsDetails_[Dict[str, Any]],\n",
    "    meta_annotations: MetaAnnotations_[\n",
    "        Dict[ArgumentName_[str], MetaAnnotation_[List[str]] | None]\n",
    "    ],\n",
    "    args_fields:Dict[str, FieldInfo]\n",
    ") -> MetaDict_[Dict[str, int]]:\n",
    "\t\"\"\"\n",
    "\tYou will want to implement this by yourself,\n",
    "\tfor your customized use.\n",
    "\t\"\"\"\n",
    "\tmeta_dict = {}\n",
    "\tfor name, arg in args_details.items():\n",
    "\t\tmeta_annotation = meta_annotations[name]\n",
    "\t\tif name != \"return\":\n",
    "\t\t\targ_type = args_fields[name].annotation\n",
    "\t\t\tif meta_annotation is not None:\n",
    "\t\t\t\tnames = get_names(meta_annotation=meta_annotation)\n",
    "\t\t\t\tif arg_type.__name__ == TensorMock.__name__:\n",
    "\t\t\t\t\tfor shape_unit, meta_name in zip(arg.shape, names):\n",
    "\t\t\t\t\t\tif meta_name not in meta_dict.keys():\n",
    "\t\t\t\t\t\t\tmeta_dict[meta_name] = shape_unit\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tif meta_dict[meta_name] != shape_unit:\n",
    "\t\t\t\t\t\t\t\traise Exception(\n",
    "\t\t\t\t\t\t\t\t\t\"Shape Units sharing same name don't have the same value.\"\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\telif arg_type in [FloatRange, IntRange]:\n",
    "\t\t\t\t\tmeta_dict[name] = arg\n",
    "\treturn meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_float_range(arg_node: ast.arg, range: List[int]):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_annotations = get_meta_annotations(forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tensor': ['b', 'c', 'h', 'w'],\n",
       " 'temperature': ['temperature >= 0', 'temperature <= 1'],\n",
       " 'depth': ['depth >= 3', 'depth <= 12'],\n",
       " 'extra_arg': None,\n",
       " 'return': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnnotationExecutor.set_functions(\n",
    "    get_meta_annotations=get_meta_annotations,\n",
    "    generate_meta_dict=generate_meta_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature >= 0': True,\n",
       " 'temperature <= 1': True,\n",
       " 'depth >= 3': True,\n",
       " 'depth <= 12': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = TensorMock.ones((4, 6, 8, 10))\n",
    "\n",
    "forward(\n",
    "    tensor = tensor,\n",
    "    temperature = 0.3,\n",
    "    depth = 10,   \n",
    ")\n",
    "\n",
    "AnnotationExecutor.last_execution_result.test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature >= 0': True,\n",
       " 'temperature <= 1': False,\n",
       " 'depth >= 3': True,\n",
       " 'depth <= 12': False}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(\n",
    "    tensor = tensor,\n",
    "    temperature = 1.2,\n",
    "    depth = 20   \n",
    ")\n",
    "\n",
    "AnnotationExecutor.last_execution_result.test_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "executable-types-beta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
